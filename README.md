# LLaMA-AES: Advanced Automated Essay Scoring with Dynamic Loss Weighting

ë³¸ í”„ë¡œì íŠ¸ëŠ” **LLaMA-3.1** ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ìë™ ì—ì„¸ì´ ì±„ì (AES) ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¨ìˆœí•œ Cross-Entropy Lossë¥¼ ë„˜ì–´, ì ìˆ˜ ì˜ˆì¸¡ì˜ ì •êµí•¨ì„ ë†’ì´ëŠ” NTL(Number Token Loss)ê³¼ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ë°˜ì˜í•˜ëŠ” EMO(Embedding-based Metric-Oriented Loss)ë¥¼ ê²°í•©í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤.

íŠ¹íˆ, í•™ìŠµ ê³¼ì •ì—ì„œ ì†ì‹¤ í•¨ìˆ˜ ê°„ì˜ ìŠ¤ì¼€ì¼ ì°¨ì´ë¥¼ ìë™ìœ¼ë¡œ ë³´ì •í•˜ëŠ” **Dynamic Loss Weighting** ê¸°ë²•ì„ ì ìš©í•˜ì—¬ ìµœì ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ“Œ ì£¼ìš” íŠ¹ì§• (Key Features)

1. **Dynamic Loss Weighting (ì„±ëŠ¥ í•µì‹¬)**
* ì„œë¡œ ë‹¤ë¥¸ ì†ì‹¤ í•¨ìˆ˜(CE, NTL, EMO) ê°„ì˜ í¬ê¸° ì°¨ì´ë¡œ ì¸í•œ í•™ìŠµ ë¶ˆê· í˜•ì„ ë§‰ê¸° ìœ„í•´, ë§¤ Stepë§ˆë‹¤ ì†ì‹¤ ë¹„ìœ¨ì— ë”°ë¼ ê°€ì¤‘ì¹˜ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.
* ê³ ì •ëœ ê°€ì¤‘ì¹˜(Static Weight)ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ ìˆ˜ë ´ì´ ë¹ ë¥´ê³  ìµœì¢… ì„±ëŠ¥(QWK)ì´ ë” ìš°ìˆ˜í•˜ê²Œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.


2. **ë³µí•© ì†ì‹¤ í•¨ìˆ˜ (Composite Loss)**
* **CE (Cross-Entropy):** ê¸°ë³¸ì ì¸ ì–¸ì–´ ëª¨ë¸ë§ í•™ìŠµ.
* **NTL (Number Token Loss):** ìˆ«ì í† í°ì˜ ê¸°ëŒ“ê°’(Expectation)ì„ ê³„ì‚°í•˜ì—¬ ì •ë‹µ ì ìˆ˜ì™€ì˜ ì˜¤ì°¨ë¥¼ ìµœì†Œí™”í•©ë‹ˆë‹¤ (MSE / Wasserstein).
* **EMO (Embedding-based Metric-Oriented Loss):** ëª¨ë¸ì´ ì˜ˆì¸¡í•œ Top-K í† í°ë“¤ì˜ ì„ë² ë”© ê°€ì¤‘ í‰ê· ê³¼ ì •ë‹µ ì„ë² ë”© ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ í•™ìŠµì— ë°˜ì˜í•©ë‹ˆë‹¤.


3. **ë©€í‹° íƒœìŠ¤í¬ í•™ìŠµ (Multi-Task Learning, MTL)**
* ì—ì„¸ì´ ì±„ì (Score)ê³¼ í”¼ë“œë°± ìƒì„±(Feedback)ì„ ë™ì‹œì— í•™ìŠµí•©ë‹ˆë‹¤.
* MTL ëª¨ë“œ ì‹œ ì ìˆ˜ ë¶€ë¶„ì€ NTLë¡œ, í”¼ë“œë°± ë¶€ë¶„ì€ EMOë¡œ ìµœì í™”ë©ë‹ˆë‹¤.


4. **Self-Consistency ë¶„ì„**
* í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ë‹¤ìˆ˜ì˜ ì‘ë‹µ(ê°œ)ì„ ìƒì„±í•˜ê³ , **Majority Vote** ë° **Average Vote**ë¥¼ í†µí•´ ì˜ˆì¸¡ ì‹ ë¢°ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.



---

## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡° (Project Structure)

```
/
â”œâ”€â”€ main_pipeline.py             # [í•™ìŠµ -> ì¶”ë¡  -> í‰ê°€] ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
â”œâ”€â”€ self_consistency.py          # Self-Consistency ìƒ˜í”Œë§ ë° ë¶„ì„ ë„êµ¬
â”œâ”€â”€ requirements.txt             # ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ëª©ë¡
â”œâ”€â”€ aes_dataset_mtl/             # í•™ìŠµ ë°ì´í„°ì…‹ (Train/Valid/Test)
â””â”€â”€ modules/
    â”œâ”€â”€ aes_dataloader.py        # ë°ì´í„° ì „ì²˜ë¦¬ ë° Collator (MTL ì§€ì›)
    â”œâ”€â”€ custom_trainer.py        # Dynamic Weightingì´ êµ¬í˜„ëœ Trainer
    â”œâ”€â”€ number_token_loss.py     # MSE ê¸°ë°˜ NTL êµ¬í˜„
    â”œâ”€â”€ wasserstein_number_token_loss.py # Wasserstein ê¸°ë°˜ NTL êµ¬í˜„
    â”œâ”€â”€ inference_module.py      # ëª¨ë¸ ì¶”ë¡  ë° CSV ì €ì¥
    â””â”€â”€ evaluate_module.py       # QWK ì ìˆ˜ ê³„ì‚° ë° í‰ê°€

```

---

## ğŸ› ï¸ ì„¤ì¹˜ ë°©ë²• (Installation)

**Python 3.10+** í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.

```bash
# 1. ê°€ìƒí™˜ê²½ ìƒì„± (ì˜ˆì‹œ)
conda create -n aes_env python=3.10 -y
conda activate aes_env

# 2. íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt

```

---

## ğŸš€ ì‚¬ìš© ë°©ë²• 1: ëª¨ë¸ í•™ìŠµ (Training Pipeline)

`main_pipeline.py`ëŠ” í•™ìŠµ(Train), ì¶”ë¡ (Inference), í‰ê°€(Evaluation)ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.

### ğŸ”¥ ì¶”ì²œ: Dynamic Weighting ëª¨ë“œ ì‹¤í–‰ (Best Performance)

ê°€ì¤‘ì¹˜ ì¸ìì— **ìŒìˆ˜ ê°’(ì˜ˆ: -1.0)**ì„ ì£¼ë©´ Dynamic Weightingì´ í™œì„±í™”ë©ë‹ˆë‹¤.

```bash
python main_pipeline.py \
    --base_model_name "meta-llama/Llama-3.1-8B-Instruct" \
    --mtl \
    --ntl_weights -1.0 \
    --emo_weights -1.0 \
    --loss_type mse \
    --device_id 0

```

* `--ntl_weights -1.0`: NTL ì†ì‹¤ì— ëŒ€í•´ ë™ì  ê°€ì¤‘ì¹˜ ì ìš©
* `--emo_weights -1.0`: EMO ì†ì‹¤ì— ëŒ€í•´ ë™ì  ê°€ì¤‘ì¹˜ ì ìš© (NTL+EMO í†µí•© ë™ì  ì¡°ì ˆ)

### ì¼ë°˜ ì‹¤í–‰ (ê³ ì • ê°€ì¤‘ì¹˜)

```bash
python main_pipeline.py \
    --base_model_name "meta-llama/Llama-3.1-8B-Instruct" \
    --mtl \
    --ntl_weights 2.0 \
    --emo_weights 0.1 \
    --device_id 0

```

### ì£¼ìš” ì¸ì ì„¤ëª…

| ì¸ì | ì„¤ëª… | ê¸°ë³¸ê°’ |
| --- | --- | --- |
| `--base_model_name` | (í•„ìˆ˜) ë² ì´ìŠ¤ ëª¨ë¸ ê²½ë¡œ ë˜ëŠ” HuggingFace ID | - |
| `--mtl` | Multi-Task Learning ë°ì´í„°ì…‹ ì‚¬ìš© ì—¬ë¶€ | `False` |
| `--ntl_weights` | NTL ê°€ì¤‘ì¹˜. **ìŒìˆ˜ ì…ë ¥ ì‹œ Dynamic Weighting í™œì„±í™”** | `2.0` |
| `--emo_weights` | EMO ê°€ì¤‘ì¹˜. **ìŒìˆ˜ ì…ë ¥ ì‹œ Dynamic Weighting í™œì„±í™”** | `0.1` |
| `--loss_type` | NTL ì†ì‹¤ í•¨ìˆ˜ ì¢…ë¥˜ (`mse` ë˜ëŠ” `was`) | `mse` |
| `--ratio` | í•™ìŠµ ë°ì´í„° ì‚¬ìš© ë¹„ìœ¨ (0.1 = 10%) | `1.0` |
| `--resume_checkpoint` | í•™ìŠµ ì¬ê°œí•  ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ | `None` |

---

## ğŸ“Š ì‚¬ìš© ë°©ë²• 2: Self-Consistency ë¶„ì„

`self_consistency.py`ëŠ” í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìƒ˜í”Œë§ ë° ì•™ìƒë¸” íš¨ê³¼ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

### 1. Run ëª¨ë“œ (ìƒ˜í”Œë§ + ë¶„ì„)

```bash
python self_consistency.py run \
    --adapter_dir "./runs/your_best_adapter" \
    --base_model_name "meta-llama/Llama-3.1-8B-Instruct" \
    --test_path "./aes_dataset_mtl/test.jsonl" \
    --max_m 50 \
    --temperature 0.7 \
    --device_id 0

```

### 2. Analyze ëª¨ë“œ (ë¶„ì„ ì „ìš©)

ì´ë¯¸ ìƒì„±ëœ ìƒ˜í”Œ JSON íŒŒì¼ì´ ìˆì„ ë•Œ ê·¸ë˜í”„ë§Œ ë‹¤ì‹œ ê·¸ë¦½ë‹ˆë‹¤.

```bash
python self_consistency.py analyze \
    --bank_path "./consistency_results/.../samples_m50_xxxx.json" \
    --test_path "./aes_dataset_mtl/test.jsonl"

```

---

## ğŸ“ ë°ì´í„°ì…‹ í¬ë§· (JSONL)

**MTL í¬ë§· ì˜ˆì‹œ:**

```json
{
  "instruction": "Evaluate the following essay...",
  "output": "4 3 4 3 4 3 4 3\n\nEssay Feedback: The essay shows..."
}

```

* `custom_trainer.py`ëŠ” `output`ì˜ **ìˆ«ì ì ìˆ˜ ë¶€ë¶„**ê³¼ **í…ìŠ¤íŠ¸ í”¼ë“œë°± ë¶€ë¶„**ì„ ìë™ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ê°ê° NTLê³¼ EMO Lossë¥¼ ì ìš©í•©ë‹ˆë‹¤.
